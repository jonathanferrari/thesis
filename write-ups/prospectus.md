# Final Prospectus

> A concise 4-5 page final written prospectus that includes the elements that you have already created in previous assignments.  
> Elements to be included
> - Motivation of the project
> - Dataset Description
> - Methodology Description
> - Proposed outcomes and  relevance of the project
> - Any possible blind spots, or ethical issues that you will address in the thesis

## [Submit here](https://bcourses.berkeley.edu/courses/1529565/assignments/8643636)
## [Document Here](https://docs.google.com/document/d/1YZ8IDTERTQ6yILkiVREMEm4FhLHryRnt98aHcQY7feQ/edit?usp=sharing)

<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=sDU-RIIs3Wq_4pUcDwWu-05zdwzqyXAFhQ3EpAK6bTA);ol{margin:0;padding:0}table td,table th{padding:0}.c8{background-color:#f9fbfd;-webkit-text-decoration-skip:none;color:#444746;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Cambria";font-style:normal}.c1{background-color:#f9fbfd;color:#444746;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Times New Roman";font-style:normal}.c3{background-color:#f9fbfd;color:#444746;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c20{background-color:#f9fbfd;-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Times New Roman"}.c2{background-color:#f9fbfd;color:#444746;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c12{background-color:#f9fbfd;-webkit-text-decoration-skip:none;color:#1155cc;font-weight:400;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt;font-family:"Cambria"}.c14{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Cambria";font-style:normal}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c0{padding-top:0pt;padding-bottom:0pt;line-height:2.0;orphans:2;widows:2;text-align:left}.c5{background-color:#f9fbfd;font-size:12pt;font-family:"Cambria";color:#444746;font-weight:400}.c10{background-color:#f9fbfd;font-size:12pt;font-family:"Times New Roman";color:#444746;font-weight:400}.c9{font-size:12pt;font-family:"Cambria";font-style:italic;font-weight:400}.c16{color:#000000;text-decoration:none;vertical-align:baseline}.c17{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c15{color:inherit;text-decoration:inherit}.c7{margin-left:36pt;text-indent:-36pt}.c18{font-style:italic}.c19{vertical-align:super}.c13{height:11pt}.c11{text-indent:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c17 doc-content"><p class="c4"><span class="c9 c16">Jonathan Ferrari</span></p><p class="c4"><span class="c9 c16">Data H195A</span></p><p class="c4"><span class="c9">November 9</span><span class="c9 c19">th</span><span class="c9 c16">, 2023</span></p><p class="c4"><span class="c9 c16">Van Dusen</span></p><p class="c6"><span class="c14">Thesis Methodology:</span></p><p class="c6"><span class="c14">Real-Time Translation of American Sign Language</span></p><p class="c6 c13"><span class="c14"></span></p><p class="c0"><span class="c8">Methods</span></p><p class="c0"><span class="c3">Data Gathering</span></p><p class="c0 c11"><span class="c5">I will have two different datasets that I will work with. The first dataset that I have been working with has already been documented and the details for it can be found in my thesis repository </span><span class="c12"><a class="c15" href="https://www.google.com/url?q=https://github.com/jonathanferrari/thesis/blob/main/write-ups/data-documentation.md&amp;sa=D&amp;source=editors&amp;ust=1702359951806792&amp;usg=AOvVaw2U_BK2d1repd6VTjIun1uw">here</a></span><span class="c2">.</span></p><p class="c0 c11"><span class="c5">My second dataset is going to be self-created. It will be a set of different videos for different signs. This is because many whole-word signs involve movement and involve much of the body. For this reason, I will need to create a dataset with multiple videos for each sign. I will do my best to recruit multiple people, and do this in front of a green screen so that I can create synthetic data so that my model is not overfitting to the background. </span></p><p class="c0"><span class="c3">Data Processing</span></p><p class="c0 c11"><span class="c5">I will be storing these image arrays as either numpy array files or I will keep them stored as jpeg files. Each observation will be read one by one, and one epoch will consist of running through all of the instances in the dataset. </span></p><p class="c0"><span class="c3">Analytics and Modeling</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There will not be too much analytics for my project, as there aren&rsquo;t necessarily summary statistics for my project, and there isn&rsquo;t much of an issue with null data, as each observation is an image or a video, and it is either all there or not at all.</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For the modeling, I will use computer vision algorithms such as Convolutional Neural Networks and ResNets. </span></p><p class="c0"><span class="c3">Documentation</span></p><p class="c0 c11"><span class="c2">Documentation will take place mainly in the repository where all of my code is hosted. I will also ensure that once I write my thesis, I will include a detailed explanation of all of my methods to ensure it is well-documented, even outside of the repository.</span></p><p class="c0"><span class="c8">Results</span></p><p class="c0"><span class="c3">Output/Outcome</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The outcome of this tool will be a live classification using a computer webcam. Each frame will be read, and there will be an option for text to be outputted on the screen, or for the text to be read aloud, in real-time.</span></p><p class="c0"><span class="c3">References</span></p><p class="c0 c7"><span class="c10">Vogler, Christian, and Dimitris N. Metaxas. </span><span class="c10 c18">Adapting Hidden Markov Models for ASL Recognition by Using Three-Dimensional Computer Vision Methods</span><span class="c10">. Nov. 2002, </span><span class="c20"><a class="c15" href="https://www.google.com/url?q=https://doi.org/10.1109/icsmc.1997.625741&amp;sa=D&amp;source=editors&amp;ust=1702359951808081&amp;usg=AOvVaw1qQuiZ85xZbkPSyt24c1Jo">https://doi.org/10.1109/icsmc.1997.625741</a></span><span class="c1">.</span></p><p class="c0 c7"><span class="c10">Kshitij Bantupalli, and Ying Xie. </span><span class="c10 c18">American Sign Language Recognition Using Deep Learning and Computer Vision</span><span class="c10">. 1 Dec. 2018, ieeexplore.ieee.org/abstract/document/8622141, </span><span class="c20"><a class="c15" href="https://www.google.com/url?q=https://doi.org/10.1109/bigdata.2018.8622141&amp;sa=D&amp;source=editors&amp;ust=1702359951808512&amp;usg=AOvVaw2mFVFBTyM0OgUizXpNBNk-">https://doi.org/10.1109/bigdata.2018.8622141</a></span><span class="c1">.</span></p><p class="c0 c7"><span class="c10">Pugeault, Nicolas, and Richard Bowden. &ldquo;Spelling It Out: Real-Time ASL Fingerspelling Recognition.&rdquo; </span><span class="c10 c18">Surrey Open Research Repository (University of Surrey)</span><span class="c10">, 1 Nov. 2011, ieeexplore.ieee.org/abstract/document/6130290, </span><span class="c20"><a class="c15" href="https://www.google.com/url?q=https://doi.org/10.1109/iccvw.2011.6130290&amp;sa=D&amp;source=editors&amp;ust=1702359951808886&amp;usg=AOvVaw0d0OdPPrAYuIHvMBj1hkVH">https://doi.org/10.1109/iccvw.2011.6130290</a></span><span class="c1">.</span></p><p class="c0 c7 c13"><span class="c1"></span></p><p class="c0"><span class="c3">Novelty</span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One new thing for my project is that it does video classification. Classically, this task has only been done using images, and I hope to implement a video-based system for classifying life feed as text. </span></p><p class="c0"><span class="c3">Contribution</span></p><p class="c0"><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;My project will contribute to the field by creating a unique approach to the problem, and by creating a tool that helps to aid in human-computer interaction.</span></p><p class="c0"><span class="c3">Biases </span></p><p class="c0"><span class="c2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One of the most important biases in computer vision will also likely affect this project, and that is how differentiating ethnicities and skin colors affect the prediction accuracy of the model. To combat this bias, I hope to collect data from multiple different people of differing skin colors and ethnicities so that I can ensure that the tool works for people from all backgrounds. </span></p></body></html>
